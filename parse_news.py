"""
–°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É –Ω–æ–≤–∏–Ω –∑ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è–º –º–æ–≤–∏
–†–µ–∞–ª—ñ–∑—É—î –∫—Ä–æ–∫–∏ 3-5 –∑ —Ç–µ—Ö–Ω—ñ—á–Ω–æ–≥–æ –∑–∞–≤–¥–∞–Ω–Ω—è
"""

import feedparser
import requests
from datetime import datetime, timedelta
from dateutil import parser as date_parser
import pytz
from typing import List, Dict, Optional
import logging
from bs4 import BeautifulSoup
import re

# –î–æ–¥–∞—î–º–æ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–≤–∏
try:
    from langdetect import detect, DetectorFactory
    # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ seed –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    DetectorFactory.seed = 0
    LANGDETECT_AVAILABLE = True
except ImportError:
    LANGDETECT_AVAILABLE = False
    logging.warning("langdetect –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ. –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–≤–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–µ.")

from config import RSS_FEEDS, UKRAINE_KEYWORDS

logger = logging.getLogger(__name__)


class NewsArticleWithLang:
    """–†–æ–∑—à–∏—Ä–µ–Ω–∞ –º–æ–¥–µ–ª—å –Ω–æ–≤–∏–Ω–Ω–æ—ó —Å—Ç–∞—Ç—Ç—ñ –∑ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è–º –º–æ–≤–∏"""
    
    def __init__(self, title: str, description: str, link: str, 
                 source: str, published_date: datetime):
        self.title = title
        self.description = description
        self.link = link
        self.source = source
        self.published_date = published_date
        
        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–æ–ª—è
        self.detected_language = None
        self.full_text = None
        self.is_ukraine_related = False
        self.gpt_classification = None
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î–º–æ –º–æ–≤—É
        self._detect_language()
    
    def _detect_language(self):
        """–í–∏–∑–Ω–∞—á–∞—î –º–æ–≤—É —Å—Ç–∞—Ç—Ç—ñ"""
        if not LANGDETECT_AVAILABLE:
            return
        
        try:
            # –û–±'—î–¥–Ω—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞ –æ–ø–∏—Å –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è
            text_for_detection = f"{self.title} {self.description}"
            
            if len(text_for_detection.strip()) > 10:
                self.detected_language = detect(text_for_detection)
                logger.debug(f"–í–∏–∑–Ω–∞—á–µ–Ω–æ –º–æ–≤—É: {self.detected_language} –¥–ª—è —Å—Ç–∞—Ç—Ç—ñ: {self.title[:50]}...")
            else:
                logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ —Ç–µ–∫—Å—Ç—É –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–≤–∏")
                
        except Exception as e:
            logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–≤–∏: {e}")
            self.detected_language = "unknown"
    
    def __str__(self):
        lang_info = f" [{self.detected_language}]" if self.detected_language else ""
        return f"{self.source}{lang_info}: {self.title} ({self.published_date})"


class SwissNewsParser:
    """–ü–∞—Ä—Å–µ—Ä —à–≤–µ–π—Ü–∞—Ä—Å—å–∫–∏—Ö –Ω–æ–≤–∏–Ω –∑ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è–º –º–æ–≤–∏"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞—î —Ç–µ–∫—Å—Ç –≤—ñ–¥ HTML —Ç–µ–≥—ñ–≤ —Ç–∞ –∑–∞–π–≤–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤"""
        if not text:
            return ""
        
        # –í–∏–¥–∞–ª—è—î–º–æ HTML —Ç–µ–≥–∏
        soup = BeautifulSoup(text, 'html.parser')
        clean_text = soup.get_text()
        
        # –û—á–∏—â–∞—î–º–æ –≤—ñ–¥ –∑–∞–π–≤–∏—Ö –ø—Ä–æ–±—ñ–ª—ñ–≤
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        return clean_text
    
    def _parse_date(self, date_string: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É –∑ RSS"""
        try:
            parsed_date = date_parser.parse(date_string)
            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ UTC —è–∫—â–æ –Ω–µ–º–∞—î timezone
            if parsed_date.tzinfo is None:
                parsed_date = pytz.UTC.localize(parsed_date)
            return parsed_date
        except Exception as e:
            logger.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ –¥–∞—Ç—É: {date_string}, –ø–æ–º–∏–ª–∫–∞: {e}")
            return None
    
    def _is_today(self, published_date: datetime) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î, —á–∏ –±—É–ª–∞ —Å—Ç–∞—Ç—Ç—è –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–∞ —Å—å–æ–≥–æ–¥–Ω—ñ"""
        if not published_date:
            return False
        
        today = datetime.now(pytz.UTC).date()
        article_date = published_date.date()
        return article_date == today
    
    def _contains_ukraine_keywords(self, text: str) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î, —á–∏ –º—ñ—Å—Ç–∏—Ç—å —Ç–µ–∫—Å—Ç –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –ø—Ä–æ –£–∫—Ä–∞—ó–Ω—É"""
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in UKRAINE_KEYWORDS)
    
    def fetch_article_text(self, url: str) -> str:
        """
        –í–∏—Ç—è–≥—É—î –ø–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—Ç—ñ –∑ –≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–∫–∏
        
        Args:
            url: URL —Å—Ç–∞—Ç—Ç—ñ
            
        Returns:
            –ü–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—Ç—ñ
        """
        try:
            logger.info(f"–û—Ç—Ä–∏–º—É—î–º–æ –ø–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç –∑: {url}")
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –í–∏–¥–∞–ª—è—î–º–æ —Å–∫—Ä–∏–ø—Ç–∏ —Ç–∞ —Å—Ç–∏–ª—ñ
            for script in soup(["script", "style", "nav", "header", "footer"]):
                script.decompose()
            
            # –°–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤
            content_selectors = {
                'swissinfo.ch': [
                    '.article__content',
                    '.article-content', 
                    '[data-testid="article-content"]',
                    '.content-main'
                ],
                'letemps.ch': [
                    '.article-content',
                    '.article__content',
                    '.content-article',
                    '.article-body'
                ],
                '20min.ch': [
                    '.article-content',
                    '.ArticleDetail_content',
                    '.content'
                ]
            }
            
            # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Å–∞–π—Ç –∑ URL
            site_key = None
            for key in content_selectors.keys():
                if key in url:
                    site_key = key
                    break
            
            content = ""
            
            # –ü—Ä–æ–±—É—î–º–æ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏ –¥–ª—è —Å–∞–π—Ç—É
            if site_key and site_key in content_selectors:
                for selector in content_selectors[site_key]:
                    elements = soup.select(selector)
                    if elements:
                        content = elements[0].get_text()
                        break
            
            # –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç, –ø—Ä–æ–±—É—î–º–æ –∑–∞–≥–∞–ª—å–Ω—ñ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏
            if not content:
                general_selectors = [
                    'article', '.article', '.post-content', '.entry-content',
                    '.content', '.main-content', 'main', '.article-body'
                ]
                
                for selector in general_selectors:
                    elements = soup.select(selector)
                    if elements:
                        content = elements[0].get_text()
                        break
            
            # –Ø–∫—â–æ –≤—Å–µ —â–µ –Ω–µ –∑–Ω–∞–π—à–ª–∏, –±–µ—Ä–µ–º–æ –≤–µ—Å—å —Ç–µ–∫—Å—Ç
            if not content:
                content = soup.get_text()
            
            # –û—á–∏—â–∞—î–º–æ —Ç–µ–∫—Å—Ç
            clean_content = self._clean_text(content)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –º—ñ–Ω—ñ–º–∞–ª—å–Ω—É –¥–æ–≤–∂–∏–Ω—É
            if len(clean_content) < 100:
                logger.warning(f"–û—Ç—Ä–∏–º–∞–Ω–æ –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç –∑ {url}")
                return ""
            
            logger.info(f"–û—Ç—Ä–∏–º–∞–Ω–æ —Ç–µ–∫—Å—Ç –¥–æ–≤–∂–∏–Ω–æ—é {len(clean_content)} —Å–∏–º–≤–æ–ª—ñ–≤")
            return clean_content
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑ {url}: {e}")
            return ""
    
    def parse_rss_feed(self, feed_url: str, source_name: str) -> List[NewsArticleWithLang]:
        """
        –ü–∞—Ä—Å–∏—Ç—å –æ–¥–Ω—É RSS-—Å—Ç—Ä—ñ—á–∫—É —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î –Ω–æ–≤–∏–Ω–∏ –∑–∞ —Å—å–æ–≥–æ–¥–Ω—ñ
        
        Args:
            feed_url: URL RSS-—Å—Ç—Ä—ñ—á–∫–∏
            source_name: –ù–∞–∑–≤–∞ –¥–∂–µ—Ä–µ–ª–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –∑–∞ —Å—å–æ–≥–æ–¥–Ω—ñ
        """
        articles = []
        
        try:
            logger.info(f"–ü–∞—Ä—Å–∏–º–æ RSS –∑ {source_name}: {feed_url}")
            feed = feedparser.parse(feed_url)
            
            if feed.bozo:
                logger.warning(f"RSS —Å—Ç—Ä—ñ—á–∫–∞ {source_name} –º–∞—î –ø–æ–º–∏–ª–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥—É")
            
            for entry in feed.entries:
                # –û—Ç—Ä–∏–º—É—î–º–æ –¥–∞—Ç—É –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó
                published_date = None
                if hasattr(entry, 'published'):
                    published_date = self._parse_date(entry.published)
                elif hasattr(entry, 'updated'):
                    published_date = self._parse_date(entry.updated)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Å—Ç–∞—Ç—Ç—ñ –Ω–µ –∑–∞ —Å—å–æ–≥–æ–¥–Ω—ñ
                if not published_date or not self._is_today(published_date):
                    continue
                
                # –°—Ç–≤–æ—Ä—é—î–º–æ –æ–±'—î–∫—Ç —Å—Ç–∞—Ç—Ç—ñ
                title = self._clean_text(getattr(entry, 'title', ''))
                description = self._clean_text(getattr(entry, 'summary', ''))
                link = getattr(entry, 'link', '')
                
                if not title or not link:
                    continue
                
                article = NewsArticleWithLang(
                    title=title,
                    description=description,
                    link=link,
                    source=source_name,
                    published_date=published_date
                )
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞
                text_to_check = f"{article.title} {article.description}"
                if self._contains_ukraine_keywords(text_to_check):
                    article.is_ukraine_related = True
                    logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç—Ç—é –ø—Ä–æ –£–∫—Ä–∞—ó–Ω—É: {article.title}")
                
                articles.append(article)
                
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥—É RSS {source_name}: {e}")
        
        return articles
    
    def parse_all_feeds_today(self) -> Dict[str, List[NewsArticleWithLang]]:
        """
        –ü–∞—Ä—Å–∏—Ç—å –≤—Å—ñ RSS-—Å—Ç—Ä—ñ—á–∫–∏ —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î –Ω–æ–≤–∏–Ω–∏ –∑–∞ —Å—å–æ–≥–æ–¥–Ω—ñ
        
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–∞—Ä—Å–∏–Ω–≥—É
        """
        all_articles = []
        ukraine_articles = []
        
        # –ü–∞—Ä—Å–∏–º–æ –≤—Å—ñ RSS-—Å—Ç—Ä—ñ—á–∫–∏
        for source_name, feed_url in RSS_FEEDS.items():
            articles = self.parse_rss_feed(feed_url, source_name)
            all_articles.extend(articles)
            
            # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Å—Ç–∞—Ç—Ç—ñ –ø—Ä–æ –£–∫—Ä–∞—ó–Ω—É
            for article in articles:
                if article.is_ukraine_related:
                    ukraine_articles.append(article)
        
        results = {
            'all_articles': all_articles,
            'ukraine_articles': ukraine_articles,
            'total_count': len(all_articles),
            'ukraine_count': len(ukraine_articles)
        }
        
        logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(all_articles)} —Å—Ç–∞—Ç–µ–π –∑–∞ —Å—å–æ–≥–æ–¥–Ω—ñ")
        logger.info(f"–ó –Ω–∏—Ö {len(ukraine_articles)} —Å—Ç–∞—Ç–µ–π –ø—Ä–æ –£–∫—Ä–∞—ó–Ω—É")
        
        return results
    
    def get_articles_with_full_text(self, articles: List[NewsArticleWithLang]) -> List[NewsArticleWithLang]:
        """
        –û—Ç—Ä–∏–º—É—î –ø–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å–ø–∏—Å–∫—É —Å—Ç–∞—Ç–µ–π
        
        Args:
            articles: –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –∑ –ø–æ–≤–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º
        """
        for article in articles:
            try:
                logger.info(f"–û—Ç—Ä–∏–º—É—î–º–æ –ø–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è: {article.title}")
                article.full_text = self.fetch_article_text(article.link)
                
                # –ù–µ–≤–µ–ª–∏–∫–∞ –∑–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏
                import time
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –¥–ª—è {article.title}: {e}")
        
        return articles


def classify_with_gpt(article: NewsArticleWithLang, gpt_processor=None) -> str:
    """
    –ö–ª–∞—Å–∏—Ñ—ñ–∫—É—î –Ω–æ–≤–∏–Ω—É —á–µ—Ä–µ–∑ GPT
    
    Args:
        article: –°—Ç–∞—Ç—Ç—è –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
        gpt_processor: GPT –ø—Ä–æ—Ü–µ—Å–æ—Ä (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
        
    Returns:
        "Ukraine-related" –∞–±–æ "Not related"
    """
    if not gpt_processor:
        logger.warning("GPT –ø—Ä–æ—Ü–µ—Å–æ—Ä –Ω–µ –Ω–∞–¥–∞–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—é")
        return "Not classified"
    
    try:
        text_for_classification = f"{article.title}\n\n{article.description}"
        if article.full_text:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–µ—Ä—à—ñ 500 —Å–∏–º–≤–æ–ª—ñ–≤ –ø–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
            text_for_classification += f"\n\n{article.full_text[:500]}..."
        
        classification = gpt_processor.classify_news(text_for_classification)
        article.gpt_classification = classification
        
        logger.info(f"GPT –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –¥–ª—è '{article.title}': {classification}")
        return classification
        
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ GPT –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó: {e}")
        return "Error"


def main():
    """–û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø–∞—Ä—Å–µ—Ä–∞"""
    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("üì∞ –ü–∞—Ä—Å–µ—Ä —à–≤–µ–π—Ü–∞—Ä—Å—å–∫–∏—Ö –Ω–æ–≤–∏–Ω –∑–∞ —Å—å–æ–≥–æ–¥–Ω—ñ")
    print("=" * 50)
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–≤–∏
    if not LANGDETECT_AVAILABLE:
        print("‚ö†Ô∏è  langdetect –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å: pip install langdetect")
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–∞—Ä—Å–µ—Ä
    parser = SwissNewsParser()
    
    # –ü–∞—Ä—Å–∏–º–æ –Ω–æ–≤–∏–Ω–∏ –∑–∞ —Å—å–æ–≥–æ–¥–Ω—ñ
    results = parser.parse_all_feeds_today()
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–∞—Ä—Å–∏–Ω–≥—É:")
    print(f"   –í—Å—å–æ–≥–æ —Å—Ç–∞—Ç–µ–π –∑–∞ —Å—å–æ–≥–æ–¥–Ω—ñ: {results['total_count']}")
    print(f"   –°—Ç–∞—Ç–µ–π –ø—Ä–æ –£–∫—Ä–∞—ó–Ω—É: {results['ukraine_count']}")
    
    # –í–∏–≤–æ–¥–∏–º–æ –≤—Å—ñ —Å—Ç–∞—Ç—Ç—ñ
    if results['all_articles']:
        print(f"\nüìã –í—Å—ñ —Å—Ç–∞—Ç—Ç—ñ –∑–∞ —Å—å–æ–≥–æ–¥–Ω—ñ:")
        for i, article in enumerate(results['all_articles'], 1):
            ukraine_mark = "üá∫üá¶" if article.is_ukraine_related else "  "
            print(f"{i:2d}. {ukraine_mark} {article}")
    
    # –î–µ—Ç–∞–ª—å–Ω–æ –≤–∏–≤–æ–¥–∏–º–æ —Å—Ç–∞—Ç—Ç—ñ –ø—Ä–æ –£–∫—Ä–∞—ó–Ω—É
    if results['ukraine_articles']:
        print(f"\nüá∫üá¶ –°—Ç–∞—Ç—Ç—ñ –ø—Ä–æ –£–∫—Ä–∞—ó–Ω—É:")
        for i, article in enumerate(results['ukraine_articles'], 1):
            print(f"\n{i}. {article.source.upper()}")
            print(f"   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.title}")
            print(f"   –ú–æ–≤–∞: {article.detected_language or '–Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ'}")
            print(f"   –û–ø–∏—Å: {article.description[:200]}...")
            print(f"   –ü–æ—Å–∏–ª–∞–Ω–Ω—è: {article.link}")
            print(f"   –î–∞—Ç–∞: {article.published_date}")
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –ø–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ç–∞—Ç–µ–π –ø—Ä–æ –£–∫—Ä–∞—ó–Ω—É
        print(f"\nüìÑ –û—Ç—Ä–∏–º—É—î–º–æ –ø–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç–µ–π...")
        ukraine_with_text = parser.get_articles_with_full_text(results['ukraine_articles'])
        
        for article in ukraine_with_text:
            if article.full_text:
                print(f"\nüì∞ –ü–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç: {article.title}")
                print(f"–î–æ–≤–∂–∏–Ω–∞: {len(article.full_text)} —Å–∏–º–≤–æ–ª—ñ–≤")
                print(f"–ü–æ—á–∞—Ç–æ–∫: {article.full_text[:300]}...")
    else:
        print("\nüîç –°—Ç–∞—Ç–µ–π –ø—Ä–æ –£–∫—Ä–∞—ó–Ω—É –∑–∞ —Å—å–æ–≥–æ–¥–Ω—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")


if __name__ == "__main__":
    main()
